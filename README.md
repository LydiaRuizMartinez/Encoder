# ğŸ§  Transformer Encoder from Scratch (PyTorch)

A **from-scratch implementation** of a **Transformer Encoder** in PyTorch, including **self-attention**, **multi-head attention**, **feed-forward networks**, positional embeddings, and a **classification head** for sequence classification tasks.

The goal of this repository is to provide a **clear, modular, and educational implementation** of the Transformer architecture, without relying on high-level abstractions such as `nn.Transformer` or external NLP libraries.

---

## âœ¨ Key Features

- ğŸ” Manual implementation of **Scaled Dot-Product Attention**
- ğŸ§© **Multi-Head Attention** with head concatenation and output projection
- ğŸ§  **Transformer Encoder Layer** with:
  - Layer Normalization
  - Residual connections
  - Feed-Forward Network using GELU
- ğŸ§¬ **Token + Positional Embeddings**
- ğŸ·ï¸ **End-to-end model for sequence classification**
- âœ… **Extensive unit tests with `pytest`** covering:
  - Tensor shapes
  - Attention behavior
  - Gradient flow and backward pass
  - Dropout behavior in train vs eval mode

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ models.py          # Transformer implementation
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_models.py     # Unit tests with pytest
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
